---
layout: post
title:  "Escalando servidor de forma simples"
date:   2024-07-15 18:30:00 +0000
categories: jekyll update
---

Quero falar sobre escalonamento de API para dar um caminho inicial por onde começar, de uma forma barata, simples, rápida e mencionar algumas ferramentas comuns que oferecem isso. E também no final, mencionar algumas dificuldades que vão aparecer após escalonarmos nossa API.

Começando, quero apresentar uma arquitetura que é comum e que é por onde começamos geralmente nossos projetos:

<img src="/assets/2024-06-24-api-arch.markdown/initial_arch.png" width="100%" />

Contextualizar cada elemneto dessa imagem:

 - Nós temos o "cliente" que pode ser uma aplicação front-end, ferramenta Postman/Insomnia ou algum outro back-end também, ou seja, alguém que vai fazer requisições para nosso back-end;

 - A "API", que será o serviço back-end que vai receber as requisições e processar o que precisa ser feito;

 - E por fim o "DB", "database" nosso banco de dados, que iremos usar para fazer uma leitura ou salvar dados.

A princípio, isso atende diversos requisições para processar mas vamos supor, que a nossa única API consegue processar 100 requisições por vez e as outras requisições acabam levando muito tempo para processar porque precisam esperar que alguma requisição finalize, como podemos resolver isso?

# Escalando

Mas, o que é "escalar" uma API? Vamos pensar, se uma aplicação nossa rodando aguenta 100 requisições, se tivermos duas, vamos conseguir aguentar 200 requisições? A grosso modo é isso mesmo! Então nossa arquitetura ficaria algo assim:

<img src="/assets/2024-06-24-api-arch.markdown/load_balancer.png" width="100%" />

Opa, agora apareceu mais alguns elementos no nosso desenho, contextualizando: 

  - NGINX, ferramenta para criar um servidor;
  - API 1, meu back-end rodando em uma porta, por exemplo, 3000;
  - API 2, meu mesmo back-end rodando em uma porta, por exemplo, 3001.

Olha que bacana, agora nós temos a mesma aplicação rodando duas vezes, agora nós podemos dobrarmos a quantidade de chamadas para o nosso serviço de 100 para 200.

Mas e o NGINX, para que ele vai servir nisso? NGINX, tem diversas funcionalidades, segurança, limitador de requisições, balanceador de carga entre os serviços, coisas que uma linguagem de programação/framework não fornece, até porque não é o propósito principal dessas ferramentas.

Vamos nos atentar no "balanceamento de carga", agora que nós temos 2 aplicações iguais rodando em portas diferente do nosso servidor, abaixo eu tenho uma configuração simples de um projeto que fiz de exemplo do NGINX:

<img src="/assets/2024-06-24-api-arch.markdown/nginx_config.png" width="100%" />

Entendendo melhor a configuração, eu tenho dois processos em NodeJS rodando, um na porta 6001 e outro na porta 6002, eu agrupo esses meus 2 serviços em uma "upstream" chamada backend e mais abaixo, no meu "server" eu estou dizendo o seguinte: "NGINX, quando alguém acessar você, eu quero que você faça o balanceamento de carga, de forma aleatória entre meus 2 processos rodando". Existem outras estratégias de balanceamento, por exemplo, o servidor que responder mais rápido mas se quiser se aprofundar nisso, vai ler por conta própria, <a href="https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/" target="_blank">aqui está a referência preguiçoso(a)</a>.

E dessa forma, eu consigo escalar minha aplicação, porém <a href="https://github.com/JoaoWitorFelipe/simples_node_nginx_load_balancer" target="_blank">no meu projeto</a> aonde eu desenvolvi esse exemplo, se formos no `package.json` é possível ver que eu preciso rodar manualmente meus 2 serviços, trazendo pra vida real e se eu precisar subir 100 serviços e balancear entre eles? É claro que não é algo viável na mão, uma forma de contornar isso é na configuração do NGINX usar um range de IPs e uma ferramenta como o Kubernetes ou PM2 (não vou falar dessas ferramentas agora para não extender muito o texto) ou alguma outra que combine com o NGINX para escalar a aplicação nesses IPs definidos, assim tendo uma escalabilidade boa e rápida caso tenha uma demanda alta. Teríamos algo mais ao menos assim no final: 


<img src="/assets/2024-06-24-api-arch.markdown/cluster_arch.png" width="100%" />

Com essas ferramentas, PM2, Kubernetes, conseguimos escalar nossas aplicações automaticamente, por exemplo, critérios muito comuns para fazer escalonamento automático:

 - Muitas requisições;
 - Aumento no uso de processamento (CPU) ou RAM.

Dessa forma, simples, rápida e barata conseguimos aumentar a perfomance da nossa aplicação, lembrando que com novas soluções, vem novos problemas, nesse último diagrama podemos ver que o nosso banco de dados vai se tornar um gargalo, pois agora ele terá uma demanda maior por conta do escalonamento do nosso servidor mas eu vou deixar isso para um outro post ou não também.