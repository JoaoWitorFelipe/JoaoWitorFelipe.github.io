---
layout: post
title:  "Escalando API de forma simples"
date:   2024-07-15 18:30:00 +0000
categories: jekyll update
---

Quero falar sobre escalonamento de API, de uma forma barata, simples, rápida e mencionar algumas ferramentas comuns que oferecem isso. E também no final, quero também levantar as novas dificuldades que vão aparecer após escalonarmos nossa API.

Começando, quero apresentar uma arquitetura que é comum e que é por onde começamos geralmente nossos projetos.

<img src="/assets/2024-06-24-api-arch.markdown/initial_arch.png" width="100%" />

Contextualizar cada elemneto dessa imagem:

 - Nós temos o "cliente" que pode ser uma aplicação front-end, ferramenta Postman/Insomnia ou algum outro back-end também, ou seja, alguém que vai fazer requisições para nosso back-end;

 - A "API", que será o serviço back-end que vai receber as requisições e processar o que precisa ser feito;

 - E por fim o "DB", "database" nosso banco de dados, que iremos para fazer uma ler ou salvar dados.

A princípio, isso atende diversos requisições para processar mas vamos supor, que a nossa única API consegue processar 100 requisições por vez e as outras requisições acabam levando muito tempo para processar porque precisam esperar que alguma requisição finalize, como podemos resolver isso?

# Escalando

Mas, o que é "escalar" uma API? Vamos pensar, se uma aplicação nossa rodando aguenta 100 requisições, se tivermos duas, vamos conseguir aguentar 200 requisições? A grosso modo é isso mesmo! Então nossa arquitetura ficaria algo assim:

<img src="/assets/2024-06-24-api-arch.markdown/load_balancer.png" width="100%" />

Opa, agora apareceu mais alguns elementos no nosso desenho, contextualizando: 

 - NGINX, é uma ferramenta para criar um servidor e possuí algumas configurações (vou mencionar mais pra frente);
  - API 1, meu back-end rodando em uma porta, por exemplo, 3000;
  - API 2, meu mesmo back-end rodando em uma porta, por exemplo, 3001.

Olha que bacana, agora nós temos a mesma aplicação rodando duas vezes, agora nós podemos dobrarmos a quantidade de chamadas para o nosso serviço de 100 para 200.

Mas e o NGINX, para que ele vai servir nisso? NGINX, tem diversas funcionalidades, segurança, limitar quantidade de requisições, balancear carga entre os serviços, coisas que uma linguagem de programação/framework não fornece, até porque não é o propósito principal dessas ferramentas.

Vamos nos atentar no "balanceamento de carga" NGINX, agora que nós temos 2 aplicações iguais rodando em portas diferente do nosso servidor, abaixo eu tenho uma configuração simples de um projeto que fiz de exemplo do NGINX:

--- Imagem do NGINX lá do meu github ---

Como podemos ver, eu defino que meu servidor vai rodar em uma porta (colocar a porta aqui), e irá balancear entre os meus 2 serviços que estão rodando.

Além do NGINX, nós temos uma ferramenta muito comum também para NodeJS, que é o PM2, o PM2 não é um servidor mas sim um gerenciador de processos. O que isso significa? Quando rodamos um "node index.js", isso é um processo que está rodando em uma porta, ou seja, o PM2 consegue gerenciar esses processos para nós, que chamamos de "cluster", do português, "grupos". O PM2 vai ajudar, a criar grupos de processos para nós, sem nos preocuparmos em como ele gerencia isso, diferente do NGINX, já que se usarmos o NGINX, vamos precisar alguma outra ferramenta que faça a gerência dos nossos processos para escalar.